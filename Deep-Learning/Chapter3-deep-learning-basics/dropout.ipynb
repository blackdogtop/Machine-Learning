{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13.2 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 丢弃所有元素\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob  # 在0, 1范围内随机生成实数 小于keep_prob则为1\n",
    "    # 输出期望改变 - 原始输出f(x)期望: f(x) 添加dropout后输出f(x)期望: p*0 + (1-p)*f(x)  = (1-p)*f(x)\n",
    "    # 故需要在训练过程处以(1-p)或在测试过程乘以(1-p)\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(16).reshape((2, 8))\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  4.  6.  0.  0.  0. 14.]\n",
       " [ 0. 18.  0.  0. 24. 26. 28.  0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 2x8 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X, W1) + b1).relu()\n",
    "    if autograd.is_training(): # 只在训练模型时使⽤丢弃法\n",
    "        H1 = dropout(H1, drop_prob1) # 在第⼀层全连接后添加丢弃层\n",
    "    H2 = (nd.dot(H1, W2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2) # 在第⼆层全连接后添加丢弃层\n",
    "    return nd.dot(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2234, train acc 0.522, test acc 0.770\n",
      "epoch 2, loss 0.6022, train acc 0.775, test acc 0.830\n",
      "epoch 3, loss 0.5074, train acc 0.814, test acc 0.835\n",
      "epoch 4, loss 0.4610, train acc 0.834, test acc 0.853\n",
      "epoch 5, loss 0.4332, train acc 0.845, test acc 0.854\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size = 5, 0.5, 256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.13.3 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1), # 在第⼀个全连接层后添加丢弃层\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2), # 在第⼆个全连接层后添加丢弃层\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1274, train acc 0.566, test acc 0.778\n",
      "epoch 2, loss 0.5590, train acc 0.790, test acc 0.822\n",
      "epoch 3, loss 0.4570, train acc 0.831, test acc 0.845\n",
      "epoch 4, loss 0.5334, train acc 0.815, test acc 0.847\n",
      "epoch 5, loss 0.4298, train acc 0.840, test acc 0.864\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2032, train acc 0.531, test acc 0.741\n",
      "epoch 2, loss 0.6153, train acc 0.768, test acc 0.825\n",
      "epoch 3, loss 0.5253, train acc 0.806, test acc 0.844\n",
      "epoch 4, loss 0.4847, train acc 0.821, test acc 0.853\n",
      "epoch 5, loss 0.4538, train acc 0.833, test acc 0.853\n"
     ]
    }
   ],
   "source": [
    "# 如果把本节中的两个丢弃概率超参数对调，会有什么结果？\n",
    "# 此处实验分别运行代码4次 且每一次dropout率对调后的模型比对调之前收敛更慢 \n",
    "# 猜测可能是因为输入层的神经元被dropout导致模型无法学习到特征从而更难拟合\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2), # 在第⼀个全连接层后添加丢弃层\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1), # 在第⼆个全连接层后添加丢弃层\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1043, train acc 0.568, test acc 0.777\n",
      "epoch 2, loss 0.5781, train acc 0.780, test acc 0.819\n",
      "epoch 3, loss 0.4791, train acc 0.825, test acc 0.854\n",
      "epoch 4, loss 0.4354, train acc 0.839, test acc 0.859\n",
      "epoch 5, loss 0.4091, train acc 0.849, test acc 0.855\n",
      "\n",
      "epoch 1, loss 1.1918, train acc 0.526, test acc 0.779\n",
      "epoch 2, loss 0.6226, train acc 0.765, test acc 0.827\n",
      "epoch 3, loss 0.5307, train acc 0.805, test acc 0.841\n",
      "epoch 4, loss 0.4803, train acc 0.823, test acc 0.849\n",
      "epoch 5, loss 0.4529, train acc 0.834, test acc 0.854\n",
      "\n",
      "\n",
      "\n",
      "epoch 1, loss 1.1103, train acc 0.562, test acc 0.793\n",
      "epoch 2, loss 0.5674, train acc 0.787, test acc 0.840\n",
      "epoch 3, loss 0.4769, train acc 0.823, test acc 0.851\n",
      "epoch 4, loss 0.6609, train acc 0.794, test acc 0.816\n",
      "epoch 5, loss 0.5014, train acc 0.818, test acc 0.856\n",
      "\n",
      "epoch 1, loss 1.1259, train acc 0.567, test acc 0.780\n",
      "epoch 2, loss 0.5897, train acc 0.781, test acc 0.838\n",
      "epoch 3, loss 0.5125, train acc 0.812, test acc 0.847\n",
      "epoch 4, loss 0.4677, train acc 0.828, test acc 0.856\n",
      "epoch 5, loss 0.4396, train acc 0.838, test acc 0.858\n",
      "\n",
      "\n",
      "\n",
      "epoch 1, loss 1.3179, train acc 0.490, test acc 0.733\n",
      "epoch 2, loss 0.6014, train acc 0.775, test acc 0.822\n",
      "epoch 3, loss 0.5046, train acc 0.815, test acc 0.848\n",
      "epoch 4, loss 0.4572, train acc 0.831, test acc 0.855\n",
      "epoch 5, loss 0.4242, train acc 0.844, test acc 0.863\n",
      "\n",
      "epoch 1, loss 1.1661, train acc 0.547, test acc 0.780\n",
      "epoch 2, loss 0.6114, train acc 0.769, test acc 0.819\n",
      "epoch 3, loss 0.5240, train acc 0.808, test acc 0.839\n",
      "epoch 4, loss 0.4767, train acc 0.826, test acc 0.843\n",
      "epoch 5, loss 0.4562, train acc 0.833, test acc 0.853\n",
      "\n",
      "\n",
      "\n",
      "epoch 1, loss 1.2358, train acc 0.516, test acc 0.772\n",
      "epoch 2, loss 0.5746, train acc 0.786, test acc 0.831\n",
      "epoch 3, loss 0.4814, train acc 0.823, test acc 0.845\n",
      "epoch 4, loss 0.4385, train acc 0.839, test acc 0.854\n",
      "epoch 5, loss 0.4086, train acc 0.849, test acc 0.863\n",
      "\n",
      "epoch 1, loss 1.1821, train acc 0.536, test acc 0.796\n",
      "epoch 2, loss 0.6287, train acc 0.764, test acc 0.827\n",
      "epoch 3, loss 0.5299, train acc 0.803, test acc 0.847\n",
      "epoch 4, loss 0.4826, train acc 0.822, test acc 0.851\n",
      "epoch 5, loss 0.4530, train acc 0.833, test acc 0.862\n",
      "\n",
      "\n",
      "\n",
      "epoch 1, loss 1.2028, train acc 0.534, test acc 0.771\n",
      "epoch 2, loss 0.5783, train acc 0.783, test acc 0.823\n",
      "epoch 3, loss 0.4827, train acc 0.821, test acc 0.849\n",
      "epoch 4, loss 0.4375, train acc 0.839, test acc 0.861\n",
      "epoch 5, loss 0.4155, train acc 0.848, test acc 0.865\n",
      "\n",
      "epoch 1, loss 1.1589, train acc 0.545, test acc 0.763\n",
      "epoch 2, loss 0.5982, train acc 0.774, test acc 0.833\n",
      "epoch 3, loss 0.5140, train acc 0.810, test acc 0.843\n",
      "epoch 4, loss 0.4787, train acc 0.824, test acc 0.853\n",
      "epoch 5, loss 0.4440, train acc 0.837, test acc 0.861\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 为了验证猜想 至改变不同的输入层dropout \n",
    "def trainWithSmallDroput():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256, activation=\"relu\"),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Dense(256, activation=\"relu\"),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Dense(10))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)\n",
    "\n",
    "def trainWithLargeDroput():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256, activation=\"relu\"),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Dense(256, activation=\"relu\"),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Dense(10))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    trainWithSmallDroput()\n",
    "    print()\n",
    "    trainWithLargeDroput()\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.3107, train acc 0.499, test acc 0.734\n",
      "epoch 2, loss 0.6193, train acc 0.766, test acc 0.821\n",
      "epoch 3, loss 0.5127, train acc 0.813, test acc 0.850\n",
      "epoch 4, loss 0.4666, train acc 0.829, test acc 0.857\n",
      "epoch 5, loss 0.4348, train acc 0.841, test acc 0.856\n",
      "epoch 6, loss 0.4146, train acc 0.848, test acc 0.865\n",
      "epoch 7, loss 0.4003, train acc 0.855, test acc 0.866\n",
      "epoch 8, loss 0.3863, train acc 0.859, test acc 0.869\n",
      "epoch 9, loss 0.3773, train acc 0.863, test acc 0.874\n",
      "epoch 10, loss 0.3647, train acc 0.866, test acc 0.862\n"
     ]
    }
   ],
   "source": [
    "# 增⼤迭代周期数，使⽤丢弃法结果。\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, 10, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.1386, train acc 0.552, test acc 0.772\n",
      "epoch 2, loss 0.5442, train acc 0.796, test acc 0.837\n",
      "epoch 3, loss 0.4511, train acc 0.832, test acc 0.852\n",
      "epoch 4, loss 0.4092, train acc 0.847, test acc 0.860\n",
      "epoch 5, loss 0.3835, train acc 0.856, test acc 0.867\n",
      "epoch 6, loss 0.3634, train acc 0.864, test acc 0.867\n",
      "epoch 7, loss 0.3519, train acc 0.869, test acc 0.872\n",
      "epoch 8, loss 0.3309, train acc 0.876, test acc 0.874\n",
      "epoch 9, loss 0.3233, train acc 0.879, test acc 0.876\n",
      "epoch 10, loss 0.3079, train acc 0.885, test acc 0.882\n"
     ]
    }
   ],
   "source": [
    "# 增⼤迭代周期数，不使⽤丢弃法结果。\n",
    "# 拟合更快\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, 10, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果将模型改得更加复杂，如增加隐藏层单元，使⽤丢弃法应对过拟合的效果是否更加明显？\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "#         nn.Dropout(0.2), # 在第⼀个全连接层后添加丢弃层\n",
    "        \n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "#         nn.Dropout(0.2), # 在第⼆个全连接层后添加丢弃层\n",
    "        \n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "#         nn.Dropout(0.5), # 在第三个全连接层后添加丢弃层\n",
    "        \n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "#         nn.Dropout(0.5), # 在第四个全连接层后添加丢弃层\n",
    "        \n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.3031, train acc 0.099, test acc 0.100\n",
      "epoch 2, loss 2.3022, train acc 0.105, test acc 0.195\n",
      "epoch 3, loss 1.6711, train acc 0.303, test acc 0.547\n",
      "epoch 4, loss 1.2419, train acc 0.485, test acc 0.660\n",
      "epoch 5, loss 0.8630, train acc 0.643, test acc 0.713\n",
      "epoch 6, loss 0.6985, train acc 0.716, test acc 0.772\n",
      "epoch 7, loss 0.6265, train acc 0.752, test acc 0.802\n",
      "epoch 8, loss 0.5746, train acc 0.784, test acc 0.839\n",
      "epoch 9, loss 0.5194, train acc 0.814, test acc 0.849\n",
      "epoch 10, loss 0.4847, train acc 0.829, test acc 0.855\n"
     ]
    }
   ],
   "source": [
    "# 使⽤丢弃法结果。\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, 10, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.3030, train acc 0.099, test acc 0.100\n",
      "epoch 2, loss 2.2200, train acc 0.129, test acc 0.209\n",
      "epoch 3, loss 1.4752, train acc 0.375, test acc 0.616\n",
      "epoch 4, loss 1.1878, train acc 0.543, test acc 0.658\n",
      "epoch 5, loss 0.8256, train acc 0.676, test acc 0.762\n",
      "epoch 6, loss 0.5957, train acc 0.760, test acc 0.789\n",
      "epoch 7, loss 0.5302, train acc 0.796, test acc 0.827\n",
      "epoch 8, loss 0.4812, train acc 0.821, test acc 0.840\n",
      "epoch 9, loss 0.4501, train acc 0.833, test acc 0.845\n",
      "epoch 10, loss 0.4195, train acc 0.845, test acc 0.852\n"
     ]
    }
   ],
   "source": [
    "# 不使⽤丢弃法结果。\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, 10, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 以本节中的模型为例，比较使用丢弃法与权重衰减的效果。如果同时使用丢弃法和权重衰减，效果会如何?\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(256, activation=\"relu\"),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2314, train acc 0.527, test acc 0.744\n",
      "epoch 2, loss 0.5997, train acc 0.778, test acc 0.831\n",
      "epoch 3, loss 0.4976, train acc 0.817, test acc 0.852\n",
      "epoch 4, loss 0.4550, train acc 0.835, test acc 0.853\n",
      "epoch 5, loss 0.4262, train acc 0.846, test acc 0.857\n",
      "epoch 6, loss 0.4080, train acc 0.852, test acc 0.851\n",
      "epoch 7, loss 0.3893, train acc 0.859, test acc 0.869\n",
      "epoch 8, loss 0.3762, train acc 0.865, test acc 0.876\n",
      "epoch 9, loss 0.3627, train acc 0.869, test acc 0.880\n",
      "epoch 10, loss 0.3544, train acc 0.870, test acc 0.876\n"
     ]
    }
   ],
   "source": [
    "# 只使用丢弃法\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, 10, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.2772, train acc 0.501, test acc 0.718\n",
      "epoch 2, loss 0.8281, train acc 0.695, test acc 0.788\n",
      "epoch 3, loss 0.7236, train acc 0.736, test acc 0.778\n",
      "epoch 4, loss 0.7605, train acc 0.724, test acc 0.792\n",
      "epoch 5, loss 0.7151, train acc 0.742, test acc 0.790\n",
      "epoch 6, loss 0.7093, train acc 0.743, test acc 0.786\n",
      "epoch 7, loss 0.6742, train acc 0.758, test acc 0.771\n",
      "epoch 8, loss 0.7685, train acc 0.725, test acc 0.797\n",
      "epoch 9, loss 0.8089, train acc 0.711, test acc 0.512\n",
      "epoch 10, loss 0.7766, train acc 0.711, test acc 0.767\n"
     ]
    }
   ],
   "source": [
    "# 同时使用丢弃法和权重衰减\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'wd': 0.01})\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, 10, batch_size, None, None, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
